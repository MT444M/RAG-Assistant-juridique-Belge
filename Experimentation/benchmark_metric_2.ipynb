{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, MilvusClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Milvus client\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles_collection_L2',\n",
       " 'articles_collection_IP',\n",
       " 'articles_collection_COSINE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_metric_types(entities, metric_types, embeddings_dim, batch_size):\n",
    "    # Dictionary to store the created collections\n",
    "    collections = {}\n",
    "\n",
    "    # Iterate over different metric types (L2, IP, COSINE)\n",
    "    for metric_type in metric_types:\n",
    "        collection_name = f\"articles_collection_{metric_type}\"\n",
    "        \n",
    "        # Check if the collection already exists and drop it if necessary\n",
    "        if client.has_collection(collection_name):\n",
    "            print(f\"Collection {collection_name} already exists. Dropping the collection...\\n\")\n",
    "            client.drop_collection(collection_name)\n",
    "            \n",
    "        # Define the fields of the collection\n",
    "        id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True)\n",
    "        text_field = FieldSchema(name=\"article\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "        reference_field = FieldSchema(name=\"reference\", dtype=DataType.VARCHAR, max_length=1000)\n",
    "        embedding_field = FieldSchema(name=\"embedding_articles\", dtype=DataType.FLOAT16_VECTOR, dim=embeddings_dim)\n",
    "        \n",
    "        # Define the schema\n",
    "        schema = CollectionSchema(fields=[id_field, text_field, reference_field, embedding_field], description=f\"Collection for {metric_type} benchmark\")\n",
    "        \n",
    "        # Create the collection\n",
    "        client.create_collection(collection_name=collection_name, schema=schema)\n",
    "        \n",
    "        # Insert all entities into the collection\n",
    "        print(f\"\\nInserting entities into collection: {collection_name}\")\n",
    "        # client.insert(data=entities, collection_name=collection_name)\n",
    "        for i in range(0, len(entities), batch_size):\n",
    "            batch = entities[i:i + batch_size]\n",
    "            try:\n",
    "                client.insert(data=batch, collection_name=collection_name)\n",
    "                print(f\"Inserted batch {i // batch_size + 1} successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during insertion for batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "        \n",
    "        # Create the index with the metric type (only using FLAT index)\n",
    "        index_params = MilvusClient.prepare_index_params()\n",
    "        index_params.add_index(\n",
    "            field_name=\"embedding_articles\",\n",
    "            metric_type=metric_type,  # Change only the metric type (L2, IP, COSINE)\n",
    "            index_type=\"FLAT\",  # Always use FLAT index\n",
    "            index_name=\"vector_index\",\n",
    "            params={}  # No additional parameters needed for FLAT index\n",
    "        )\n",
    "        \n",
    "        # Create the index on the collection after inserting the entities\n",
    "        try:\n",
    "            client.create_index(\n",
    "                collection_name=collection_name,\n",
    "                index_params=index_params,\n",
    "                sync=True  # Wait for index creation to complete\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create an index on collection: {collection_name}\")\n",
    "            print(e)\n",
    "            continue  # Skip this index and continue with the next one\n",
    "\n",
    "        # Store the created collection in the dictionary\n",
    "        collections[metric_type] = collection_name\n",
    "\n",
    "    print(\"\\n\\nBenchmark completed for all metric types.\")\n",
    "    # Return the dictionary containing the collections\n",
    "    return collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the JSON file\n",
    "with open('embeddings.json', 'r', encoding='utf-8') as f:\n",
    "    loaded_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_file(filepath: str) -> Dict[int, np.ndarray]:\n",
    "    try:\n",
    "        # Load the npz file\n",
    "        data = np.load(filepath)\n",
    "        \n",
    "        # Convert arrays back to dictionary\n",
    "        embeddings_dict = {\n",
    "            int(id_): emb for id_, emb in zip(data['ids'], data['embeddings'])\n",
    "        }\n",
    "        \n",
    "        print(f\"Successfully loaded {len(embeddings_dict)} embeddings\")\n",
    "        \n",
    "        return embeddings_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 22633 embeddings\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_embeddings_from_file('embeddings_bel.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv('articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_embeddings_in_chunks(\n",
    "    df_articles: pd.DataFrame,\n",
    "    loaded_embeddings: Dict[int, np.ndarray],\n",
    "    chunk_size: int = 1000\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process embeddings in chunks to optimize memory usage and maintain consistent performance.\n",
    "    \n",
    "    Args:\n",
    "        df_articles: DataFrame containing article information\n",
    "        loaded_embeddings: Dictionary mapping article IDs to their embeddings\n",
    "        chunk_size: Number of rows to process in each chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of processed entities\n",
    "    \"\"\"\n",
    "    # Initialize empty list for all entities\n",
    "    all_entities = []\n",
    "    \n",
    "    # Calculate total number of chunks\n",
    "    total_chunks = (len(df_articles) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    # Process data in chunks with progress bar\n",
    "    with tqdm(total=len(df_articles), desc=\"Processing entities\") as pbar:\n",
    "        for chunk_start in range(0, len(df_articles), chunk_size):\n",
    "            # Get chunk of dataframe\n",
    "            chunk_end = min(chunk_start + chunk_size, len(df_articles))\n",
    "            df_chunk = df_articles.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            # Process chunk\n",
    "            chunk_entities = [\n",
    "                {\n",
    "                    \"id\": row['id'],\n",
    "                    \"article\": row['article'],\n",
    "                    \"reference\": row['reference'],\n",
    "                    \"embedding_articles\": np.array(loaded_embeddings[row['id']], dtype=np.float16)\n",
    "                }\n",
    "                for _, row in df_chunk.iterrows()\n",
    "                if row['id'] in loaded_embeddings\n",
    "            ]\n",
    "            \n",
    "            # Extend all_entities with chunk results\n",
    "            all_entities.extend(chunk_entities)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(df_chunk))\n",
    "            \n",
    "            # Optional: Clear memory\n",
    "            del chunk_entities\n",
    "    \n",
    "    print(f\"Processed {len(all_entities)} entities\")\n",
    "    \n",
    "    return all_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a2ef5b01b457fa359453e91e4e58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing entities:   0%|          | 0/22633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22633 entities\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process in chunks\n",
    "processed_entities = process_embeddings_in_chunks(\n",
    "    df_articles=df_articles,\n",
    "    loaded_embeddings=embeddings,\n",
    "    chunk_size=1000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection articles_collection_L2 already exists. Dropping the collection...\n",
      "\n",
      "\n",
      "Inserting entities into collection: articles_collection_L2\n",
      "Inserted batch 1 successfully.\n",
      "Inserted batch 2 successfully.\n",
      "Inserted batch 3 successfully.\n",
      "Inserted batch 4 successfully.\n",
      "Inserted batch 5 successfully.\n",
      "Inserted batch 6 successfully.\n",
      "Inserted batch 7 successfully.\n",
      "Inserted batch 8 successfully.\n",
      "Inserted batch 9 successfully.\n",
      "Inserted batch 10 successfully.\n",
      "Inserted batch 11 successfully.\n",
      "Inserted batch 12 successfully.\n",
      "Inserted batch 13 successfully.\n",
      "Inserted batch 14 successfully.\n",
      "Inserted batch 15 successfully.\n",
      "Inserted batch 16 successfully.\n",
      "Inserted batch 17 successfully.\n",
      "Inserted batch 18 successfully.\n",
      "Inserted batch 19 successfully.\n",
      "Inserted batch 20 successfully.\n",
      "Inserted batch 21 successfully.\n",
      "Inserted batch 22 successfully.\n",
      "Inserted batch 23 successfully.\n",
      "Collection articles_collection_IP already exists. Dropping the collection...\n",
      "\n",
      "\n",
      "Inserting entities into collection: articles_collection_IP\n",
      "Inserted batch 1 successfully.\n",
      "Inserted batch 2 successfully.\n",
      "Inserted batch 3 successfully.\n",
      "Inserted batch 4 successfully.\n",
      "Inserted batch 5 successfully.\n",
      "Inserted batch 6 successfully.\n",
      "Inserted batch 7 successfully.\n",
      "Inserted batch 8 successfully.\n",
      "Inserted batch 9 successfully.\n",
      "Inserted batch 10 successfully.\n",
      "Inserted batch 11 successfully.\n",
      "Inserted batch 12 successfully.\n",
      "Inserted batch 13 successfully.\n",
      "Inserted batch 14 successfully.\n",
      "Inserted batch 15 successfully.\n",
      "Inserted batch 16 successfully.\n",
      "Inserted batch 17 successfully.\n",
      "Inserted batch 18 successfully.\n",
      "Inserted batch 19 successfully.\n",
      "Inserted batch 20 successfully.\n",
      "Inserted batch 21 successfully.\n",
      "Inserted batch 22 successfully.\n",
      "Inserted batch 23 successfully.\n",
      "Collection articles_collection_COSINE already exists. Dropping the collection...\n",
      "\n",
      "\n",
      "Inserting entities into collection: articles_collection_COSINE\n",
      "Inserted batch 1 successfully.\n",
      "Inserted batch 2 successfully.\n",
      "Inserted batch 3 successfully.\n",
      "Inserted batch 4 successfully.\n",
      "Inserted batch 5 successfully.\n",
      "Inserted batch 6 successfully.\n",
      "Inserted batch 7 successfully.\n",
      "Inserted batch 8 successfully.\n",
      "Inserted batch 9 successfully.\n",
      "Inserted batch 10 successfully.\n",
      "Inserted batch 11 successfully.\n",
      "Inserted batch 12 successfully.\n",
      "Inserted batch 13 successfully.\n",
      "Inserted batch 14 successfully.\n",
      "Inserted batch 15 successfully.\n",
      "Inserted batch 16 successfully.\n",
      "Inserted batch 17 successfully.\n",
      "Inserted batch 18 successfully.\n",
      "Inserted batch 19 successfully.\n",
      "Inserted batch 20 successfully.\n",
      "Inserted batch 21 successfully.\n",
      "Inserted batch 22 successfully.\n",
      "Inserted batch 23 successfully.\n",
      "\n",
      "\n",
      "Benchmark completed for all metric types.\n"
     ]
    }
   ],
   "source": [
    "metric_types = [\"L2\", \"IP\", \"COSINE\"]\n",
    "index_types = [\"FLAT\"]\n",
    "embeddings_dim = 1024  \n",
    "\n",
    "collections = benchmark_metric_types(processed_entities, metric_types, embeddings_dim, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_bel = SentenceTransformer('Lajavaness/bilingual-embedding-large', trust_remote_code=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dcd00ee2ec4e0bb8167c1d2a98322b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mthia\\anaconda3\\envs\\AI_App\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  colbert_state_dict = torch.load(os.path.join(model_dir, 'colbert_linear.pt'), map_location='cpu')\n",
      "c:\\Users\\mthia\\anaconda3\\envs\\AI_App\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sparse_state_dict = torch.load(os.path.join(model_dir, 'sparse_linear.pt'), map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "bge_m3 = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True, \n",
    "                       device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(article):\n",
    "    embedding = bge_m3.encode([article], batch_size=12, max_length=8*1024)[\"dense_vecs\"]\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import torch\n",
    "\n",
    "def generate_embedding(\n",
    "    article: Union[str, List[str]],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    "    max_length: int = 512,\n",
    "    device: str = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings using SentenceTransformer model\n",
    "    \n",
    "    Args:\n",
    "        article: Single article or list of articles\n",
    "        model: SentenceTransformer model instance\n",
    "        batch_size: Batch size for processing\n",
    "        max_length: Maximum sequence length\n",
    "        device: Computing device ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of embeddings\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Ensure model is on correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Convert single article to list if necessary\n",
    "    if isinstance(article, str):\n",
    "        articles = [article]\n",
    "    else:\n",
    "        articles = article\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(\n",
    "            articles,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,  # Normalize for cosine similarity\n",
    "            max_length=max_length,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # Convert embeddings to float16\n",
    "    embeddings = embeddings.astype(np.float16)\n",
    "    \n",
    "    # Return single embedding if input was single article\n",
    "    if isinstance(article, str):\n",
    "        return embeddings[0]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('questions_train.csv')\n",
    "\n",
    "#concat df_questions['question'] + df_questions['extra_description']\n",
    "\n",
    "df_questions['complet_question'] = df_questions['question'] + df_questions['extra_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For single article\n",
    "# article = \"Your article text here\"\n",
    "query_vector = generate_embedding(\n",
    "    article= df_questions['complet_question'].iloc[0],\n",
    "    model=model_bel,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# # For multiple articles\n",
    "# articles = [\"Article 1\", \"Article 2\", \"Article 3\"]\n",
    "# embeddings = generate_embedding(\n",
    "#     article=articles,\n",
    "#     model=model_bel,\n",
    "#     batch_size=32\n",
    "# )\n",
    "\n",
    "# # Process DataFrame column\n",
    "# df_embeddings = generate_embedding(\n",
    "#     article=df_articles['articles'].tolist(),\n",
    "#     model=model_bel,\n",
    "#     batch_size=32\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03096 , -0.004353,  0.03195 , ..., -0.0366  , -0.02815 ,\n",
       "       -0.0423  ], dtype=float16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('questions_train.csv')\n",
    "\n",
    "#concat df_questions['question'] + df_questions['extra_description']\n",
    "\n",
    "df_questions['complet_question'] = df_questions['question'] + df_questions['extra_description']\n",
    "\n",
    "# Vecteur de la question à tester\n",
    "query_vector = generate_embedding(df_questions['complet_question'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing collection: articles_collection_L2 with metric type: L2\n",
      "Index info for L2: {'index_type': 'FLAT', 'metric_type': 'L2', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for L2:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.626326322555542,\n",
      "      \"entity\": {\n",
      "         \"id\": 18554,\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22233,\n",
      "      \"distance\": 0.7632614374160767,\n",
      "      \"entity\": {\n",
      "         \"id\": 22233,\n",
      "         \"reference\": \"Art. X.5-9, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.8184471726417542,\n",
      "      \"entity\": {\n",
      "         \"id\": 22225,\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   }\n",
      "]\n",
      "Testing collection: articles_collection_IP with metric type: IP\n",
      "Index info for IP: {'index_type': 'FLAT', 'metric_type': 'IP', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for IP:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.6871737241744995,\n",
      "      \"entity\": {\n",
      "         \"id\": 18554,\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22233,\n",
      "      \"distance\": 0.6182908415794373,\n",
      "      \"entity\": {\n",
      "         \"id\": 22233,\n",
      "         \"reference\": \"Art. X.5-9, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.5909006595611572,\n",
      "      \"entity\": {\n",
      "         \"id\": 22225,\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   }\n",
      "]\n",
      "Testing collection: articles_collection_COSINE with metric type: COSINE\n",
      "Index info for COSINE: {'index_type': 'FLAT', 'metric_type': 'COSINE', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for COSINE:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.6869423985481262,\n",
      "      \"entity\": {\n",
      "         \"id\": 18554,\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22233,\n",
      "      \"distance\": 0.6183393597602844,\n",
      "      \"entity\": {\n",
      "         \"id\": 22233,\n",
      "         \"reference\": \"Art. X.5-9, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.5908273458480835,\n",
      "      \"entity\": {\n",
      "         \"id\": 22225,\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paramètres de recherche uniquement pour FLAT, en variant les métriques\n",
    "search_params = {\n",
    "    'L2': {\"metric_type\": \"L2\", \"params\": {}},      # Distance euclidienne\n",
    "    'COSINE': {\"metric_type\": \"COSINE\", \"params\": {}},  # Distance cosinus\n",
    "    'IP': {\"metric_type\": \"IP\", \"params\": {}},      # Produit scalaire\n",
    "}\n",
    "\n",
    "# Résultats de performance\n",
    "performance_results = {}\n",
    "\n",
    "# Boucle pour tester chaque métrique\n",
    "for metric_type, collection_name in collections.items():\n",
    "    print(f\"Testing collection: {collection_name} with metric type: {metric_type}\")\n",
    "\n",
    "    # Charger les partitions pour la collection\n",
    "    client.load_collection(collection_name=collection_name)\n",
    "\n",
    "    # Récupérer les informations sur l'index\n",
    "    index_info = client.describe_index(collection_name=collection_name, index_name=\"vector_index\")\n",
    "    print(f\"Index info for {metric_type}: {index_info}\")\n",
    "\n",
    "    # Effectuer la recherche avec la métrique correspondante\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=3,\n",
    "        search_params=search_params[metric_type],  # Utiliser les paramètres de la métrique en cours\n",
    "        output_fields=['id', 'reference']\n",
    "    )\n",
    "\n",
    "    # Formater et afficher les résultats\n",
    "    formatted_result = json.dumps(search_results[0], indent=3, ensure_ascii=False)\n",
    "    print(f\"Results for {metric_type}:\\n{formatted_result}\")\n",
    "    \n",
    "    # Enregistrer les résultats dans le dictionnaire\n",
    "    performance_results[metric_type] = search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 22225,22226,22227,22228,22229,22230,22231,22232,22233,22234 \n",
      "\n",
      "metric Type: L2 - results: [18554, 22233, 22225] - distances: [0.626326322555542, 0.7632614374160767, 0.8184471726417542]\n",
      "\n",
      "\n",
      "metric Type: IP - results: [18554, 22233, 22225] - distances: [0.6871737241744995, 0.6182908415794373, 0.5909006595611572]\n",
      "\n",
      "\n",
      "metric Type: COSINE - results: [18554, 22233, 22225] - distances: [0.6869423985481262, 0.6183393597602844, 0.5908273458480835]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = df_questions['article_ids'].iloc[0]\n",
    "print(f\"Labels: {labels} \\n\")\n",
    "\n",
    "\n",
    "# Affichage des résultats pour chaque index\n",
    "for index_type, result in performance_results.items():\n",
    "    list_ids = [result[\"entity\"][\"id\"] for result in result[0]]\n",
    "    list_prods = [result[\"distance\"] for result in result[0]]\n",
    "    print(f\"metric Type: {index_type} - results: {list_ids} - distances: {list_prods}\")\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing collection: articles_collection_L2 with metric type: L2\n",
      "Index info for L2: {'index_type': 'FLAT', 'metric_type': 'L2', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for L2:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.6798719167709351,\n",
      "      \"entity\": {\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\",\n",
      "         \"id\": 18554\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 21092,\n",
      "      \"distance\": 0.8355220556259155,\n",
      "      \"entity\": {\n",
      "         \"reference\": \"Art. I.4-50, Code du Bien-être au Travail (Livre Ier, Titre 4, Chapitre V, Section 2)\",\n",
      "         \"id\": 21092\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.8465765714645386,\n",
      "      \"entity\": {\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\",\n",
      "         \"id\": 22225\n",
      "      }\n",
      "   }\n",
      "]\n",
      "Testing collection: articles_collection_IP with metric type: IP\n",
      "Index info for IP: {'index_type': 'FLAT', 'metric_type': 'IP', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for IP:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.6600538492202759,\n",
      "      \"entity\": {\n",
      "         \"id\": 18554,\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 21092,\n",
      "      \"distance\": 0.5822518467903137,\n",
      "      \"entity\": {\n",
      "         \"id\": 21092,\n",
      "         \"reference\": \"Art. I.4-50, Code du Bien-être au Travail (Livre Ier, Titre 4, Chapitre V, Section 2)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.5766925811767578,\n",
      "      \"entity\": {\n",
      "         \"id\": 22225,\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   }\n",
      "]\n",
      "Testing collection: articles_collection_COSINE with metric type: COSINE\n",
      "Index info for COSINE: {'index_type': 'FLAT', 'metric_type': 'COSINE', 'field_name': 'embedding_articles', 'index_name': 'vector_index', 'total_rows': 22633, 'indexed_rows': 22633, 'pending_index_rows': 0, 'state': 'Finished'}\n",
      "Results for COSINE:\n",
      "[\n",
      "   {\n",
      "      \"id\": 18554,\n",
      "      \"distance\": 0.660060465335846,\n",
      "      \"entity\": {\n",
      "         \"id\": 18554,\n",
      "         \"reference\": \"Art. 392, Code de la Fonction Publique Wallonne (Livre III, Chapitre IV)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 21092,\n",
      "      \"distance\": 0.582244336605072,\n",
      "      \"entity\": {\n",
      "         \"id\": 21092,\n",
      "         \"reference\": \"Art. I.4-50, Code du Bien-être au Travail (Livre Ier, Titre 4, Chapitre V, Section 2)\"\n",
      "      }\n",
      "   },\n",
      "   {\n",
      "      \"id\": 22225,\n",
      "      \"distance\": 0.5767034888267517,\n",
      "      \"entity\": {\n",
      "         \"id\": 22225,\n",
      "         \"reference\": \"Art. X.5-1, Code du Bien-être au Travail (Livre X, Titre 5)\"\n",
      "      }\n",
      "   }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paramètres de recherche uniquement pour FLAT, en variant les métriques\n",
    "search_params = {\n",
    "    'L2': {\"metric_type\": \"L2\", \"params\": {}},      # Distance euclidienne\n",
    "    'COSINE': {\"metric_type\": \"COSINE\", \"params\": {}},  # Distance cosinus\n",
    "    'IP': {\"metric_type\": \"IP\", \"params\": {}},      # Produit scalaire\n",
    "}\n",
    "\n",
    "# Résultats de performance\n",
    "performance_results = {}\n",
    "\n",
    "# Boucle pour tester chaque métrique\n",
    "for metric_type, collection_name in collections.items():\n",
    "    print(f\"Testing collection: {collection_name} with metric type: {metric_type}\")\n",
    "\n",
    "    # Charger les partitions pour la collection\n",
    "    client.load_collection(collection_name=collection_name)\n",
    "\n",
    "    # Récupérer les informations sur l'index\n",
    "    index_info = client.describe_index(collection_name=collection_name, index_name=\"vector_index\")\n",
    "    print(f\"Index info for {metric_type}: {index_info}\")\n",
    "\n",
    "    # Effectuer la recherche avec la métrique correspondante\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=3,\n",
    "        search_params=search_params[metric_type],  # Utiliser les paramètres de la métrique en cours\n",
    "        output_fields=['id', 'reference']\n",
    "    )\n",
    "\n",
    "    # Formater et afficher les résultats\n",
    "    formatted_result = json.dumps(search_results[0], indent=3, ensure_ascii=False)\n",
    "    print(f\"Results for {metric_type}:\\n{formatted_result}\")\n",
    "    \n",
    "    # Enregistrer les résultats dans le dictionnaire\n",
    "    performance_results[metric_type] = search_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_App",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
