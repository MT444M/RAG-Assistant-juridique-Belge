{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('questions_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids_labels = []\n",
    "for article_ids in df_questions['article_ids']:\n",
    "    all_ids_labels.append(article_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# Load the predictions from the JSON file\n",
    "with gzip.open('all_predictions_RSChunk.json.gz', 'rt', encoding='utf-8') as f:\n",
    "    all_predictions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = []\n",
    "predicted_article = []\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    predicted_ids.append(all_predictions[str(i)]['predicted_ids'])\n",
    "    predicted_article.append(all_predictions[str(i)]['predictions_articles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RERANKING FUNCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def optimize_reranking(\n",
    "    predicted_ids: List[List[int]],\n",
    "    predicted_articles: List[List[str]],\n",
    "    questions: List[str],\n",
    "    ids_labels: List[str],\n",
    "    bge_m3,\n",
    "    w_d: float = 0.4,\n",
    "    w_s: float = 0.2,\n",
    "    w_c: float = 0.4,\n",
    "    batch_size: int = 12,\n",
    "    max_length: int = 512\n",
    ") -> List[List[int]]:\n",
    "    \n",
    "    def process_batch(question: str, articles: List[str]) -> np.ndarray:\n",
    "        \"\"\"Process a single question and its candidate articles using compute_score\"\"\"\n",
    "        # Create sentence pairs for the question and all articles\n",
    "        sentence_pairs = [[question, article] for article in articles]\n",
    "        \n",
    "        # Compute scores using the official method\n",
    "        scores = bge_m3.compute_score(\n",
    "            sentence_pairs,\n",
    "            max_passage_length=max_length,\n",
    "            weights_for_different_modes=[w_d, w_s, w_c]  # [dense, sparse, colbert]\n",
    "        )\n",
    "        \n",
    "        # Return the combined scores\n",
    "        return scores['colbert+sparse+dense']\n",
    "\n",
    "    # Process each question and its candidate articles\n",
    "    reranked_predictions = []\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for q, arts, preds in tqdm(zip(questions, predicted_articles, predicted_ids), \n",
    "                              total=len(questions), \n",
    "                              desc=\"Processing questions\"):\n",
    "        try:\n",
    "            # Process articles in batches if there are many\n",
    "            if len(arts) > batch_size:\n",
    "                all_scores = []\n",
    "                for i in range(0, len(arts), batch_size):\n",
    "                    batch_arts = arts[i:i + batch_size]\n",
    "                    batch_scores = process_batch(q, batch_arts)\n",
    "                    all_scores.extend(batch_scores)\n",
    "                similarities = np.array(all_scores)\n",
    "            else:\n",
    "                # Process all articles at once if within batch size\n",
    "                similarities = process_batch(q, arts)\n",
    "            \n",
    "            # Sort predictions based on similarities\n",
    "            reranked_indices = np.argsort(-similarities)\n",
    "            reranked_pred = [preds[idx] for idx in reranked_indices]\n",
    "            reranked_predictions.append(reranked_pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {q[:100]}...\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            # In case of error, keep original ordering\n",
    "            reranked_predictions.append(preds)\n",
    "    \n",
    "    return reranked_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6dd8ed681644549401f90244219403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mthia\\anaconda3\\envs\\AI_App\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  colbert_state_dict = torch.load(os.path.join(model_dir, 'colbert_linear.pt'), map_location='cpu')\n",
      "c:\\Users\\mthia\\anaconda3\\envs\\AI_App\\Lib\\site-packages\\FlagEmbedding\\BGE_M3\\modeling.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sparse_state_dict = torch.load(os.path.join(model_dir, 'sparse_linear.pt'), map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "bge_m3 = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True, \n",
    "                       device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 886/886 [42:18<00:00,  2.87s/it]  \n"
     ]
    }
   ],
   "source": [
    "reranked_preds = optimize_reranking(\n",
    "    predicted_ids=predicted_ids,\n",
    "    predicted_articles=predicted_article,\n",
    "    questions=df_questions['question'].tolist(),\n",
    "    ids_labels=all_ids_labels,\n",
    "    bge_m3=bge_m3,\n",
    "    w_d=0.4,\n",
    "    w_s=0.2,\n",
    "    w_c=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval_Retrieval(all_predictions, articles_ids, top_k=20):\n",
    "    # Assure que les articles_ids sont bien sous forme de liste d'ID (int) pour chaque requête\n",
    "    articles_ids = [list(map(int, ids.split(','))) for ids in articles_ids]\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    average_precisions = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    # Nombre total de questions\n",
    "    Q = len(all_predictions)\n",
    "\n",
    "    # Calcul des métriques pour chaque ensemble de prédictions\n",
    "    for preds, true_ids in zip(all_predictions, articles_ids):\n",
    "        # Limiter les prédictions à top_k résultats\n",
    "        preds = preds[:top_k]\n",
    "        \n",
    "        # Convertir les prédictions en set pour faciliter les calculs\n",
    "        preds_set = set(preds)\n",
    "        true_set = set(true_ids)\n",
    "\n",
    "        # Calcul des True Positives (TP), False Positives (FP), et False Negatives (FN)\n",
    "        tp = len(preds_set & true_set) # intersection (in both)\n",
    "        fp = len(preds_set - true_set)  # Difference (in pred but not in true)\n",
    "        fn = len(true_set - preds_set)  #Difference (in true but not in pred)\n",
    "\n",
    "        # Calcul Precision, Recall, F1-Score\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Calcul de l'Average Precision (AP)\n",
    "        ap = 0\n",
    "        relevant_count = 0\n",
    "        for rank, pred in enumerate(preds, 1):  # rank starts at 1\n",
    "            if pred in true_set:\n",
    "                relevant_count += 1\n",
    "                ap += relevant_count / rank\n",
    "        ap /= len(true_set) if len(true_set) > 0 else 1\n",
    "        average_precisions.append(ap)\n",
    "\n",
    "        # Calcul Mean Reciprocal Rank (MRR)\n",
    "        mrr = 0\n",
    "        for rank, pred in enumerate(preds, 1):\n",
    "            if pred in true_set:\n",
    "                mrr = 1 / rank\n",
    "                break\n",
    "        reciprocal_ranks.append(mrr)\n",
    "\n",
    "    # Calcul des métriques globales\n",
    "    mean_precision = sum(precisions) / Q\n",
    "    mean_recall = sum(recalls) / Q\n",
    "    mean_f1 = sum(f1_scores) / Q\n",
    "    mean_ap = sum(average_precisions) / Q\n",
    "    mean_mrr = sum(reciprocal_ranks) / Q\n",
    "\n",
    "    # Retourner les métriques sous forme de dictionnaire\n",
    "    return {\n",
    "        \"mean_precision\": mean_precision,\n",
    "        \"mean_recall\": mean_recall,\n",
    "        \"mean_f1_score\": mean_f1,\n",
    "        \"MAP\": mean_ap,\n",
    "        \"MRR\": mean_mrr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_precision': 0.20786305492851767,\n",
       " 'mean_recall': 0.2296581251630902,\n",
       " 'mean_f1_score': 0.17475715458591964,\n",
       " 'MAP': 0.2194387862703324,\n",
       " 'MRR': 0.3502633559066967}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate results\n",
    "Eval_Retrieval(reranked_preds, all_ids_labels, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Precision (0.056): Only about 5.6% of retrieved documents are relevant\n",
    "Mean Recall (0.228): Only about 22.8% of relevant documents are being retrieved\n",
    "F1 Score (0.066): Very low harmonic mean of precision and recall\n",
    "MAP (0.076): Low mean average precision indicates poor ranking of relevant documents\n",
    "MRR (0.182): First relevant document appears, on average, at around position 5-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_App",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
